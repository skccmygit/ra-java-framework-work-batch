<?xml version="1.0" encoding="UTF-8"?>

<configuration>
    <springProperty scope="context" name="LOG_LEVEL" source="logging.level.root"/>
    <springProperty scope="context" name="APP_NAME" source="spring.application.name"/>
    <springProperty scope="context" name="BOOTSTRAP_SERVERS" source="spring.kafka.producer.bootstrap-servers"/>

    <property name="LOG_FILE" value="/data01/COM-ASSET/apilog/${APP_NAME}/${HOSTNAME}.log"/>
    <property name="SQL_LOG_FILE" value="/data01/COM-ASSET/sqltrace/${APP_NAME}/${HOSTNAME}.trc"/>

    <property name="TOPIC" value="ONM-T-LOG-API"/>
    <property name="PRODUCER_CONFIG" value="bootstrap.servers=${BOOTSTRAP_SERVERS}"/>

    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>${CONSOLE_LOG_CHARSET}</charset>
        </encoder>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <encoder>
            <pattern>${FILE_LOG_PATTERN}</pattern>
            <charset>${FILE_LOG_CHARSET}</charset>
        </encoder>
        <file>${LOG_FILE}</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.%i</fileNamePattern>
            <maxHistory>30</maxHistory>
            <maxFileSize>100MB</maxFileSize>
            <totalSizeCap>100GB</totalSizeCap>
        </rollingPolicy>
    </appender>

    <appender name="TRACE" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder>
            <pattern>%msg%n</pattern>
            <charset>${FILE_LOG_CHARSET}</charset>
        </encoder>
        <topic>${TOPIC}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <!-- bootstrap.servers is the only mandatory producerConfig -->
        <producerConfig>${PRODUCER_CONFIG}</producerConfig>
        <!-- this is the fallback appender if kafka is not available. -->
        <appender-ref ref="CONSOLE"/>
    </appender>

    <appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
        <!-- if neverBlock is set to true, the async appender discards messages when its internal queue is full -->
        <neverBlock>true</neverBlock>
        <appender-ref ref="TRACE"/>
    </appender>

    <logger name="kr.co.skcc.base.com.common.trace.RequestTraceFilter" level="INFO" additivity="false">
        <appender-ref ref="ASYNC"/>
    </logger>

    <appender name="SQL" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${SQL_LOG_FILE}</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <fileNamePattern>${SQL_LOG_FILE}.%d{yyyy-MM-dd}.%i</fileNamePattern>
            <maxHistory>30</maxHistory>
            <maxFileSize>100MB</maxFileSize>
            <totalSizeCap>100GB</totalSizeCap>
        </rollingPolicy>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>DENY</onMatch>
            <onMismatch>ACCEPT</onMismatch>
        </filter>
        <encoder name="enc" class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="kr.co.skcc.base.com.common.trace.PatternLayoutWithUserContext">
                <param name="Pattern" value="%d{yyyy-MM-dd HH:mm:ss};%loginID;%clientIP;%replace(%msg){'\n|\r', ' '}%n"/>
            </layout>
            <charset>${FILE_LOG_CHARSET}</charset>
        </encoder>
    </appender>

    <logger name="jdbc.sqlonly" level="INFO" additivity="false">
        <appender-ref ref="SQL"/>
    </logger>

    <root level="${LOG_LEVEL}">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="FILE"/>
    </root>
</configuration>